{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atrial Fibrillation Detection\n",
    "\n",
    "This project aims to detect periods of AF from preprocessed ECG data by detecting irregular R-R intervals in the ECG signals. Different Machine Learning Classifiers are used to build an ML model using the given data. The performance of these various classifiers are analysed to determine the best classifier for this data. This is an interactive notebook that displays the results of our work. The entire source code can be found at the [GitHub](https://github.com/rushvanth/AFDetection) repository. \n",
    "\n",
    "It is highly recommended to run this Notebook in [Colab](https://colab.research.google.com/) in a GPU backed runtime. This drastically reduces run times while training models. In some cases, training time can be reduced from >5 hours (CPU processing - i7 1135G7) to ~4 minutes(GPU processing - NVIDIA T4). \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rushvanth/AFDetection/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAPIDS Setup on Colab\n",
    "\n",
    "Borrowed from [Rapids.ai](https://colab.research.google.com/drive/1rY7Ln6rEE1pOlfSHCYOVaqt8OvDO35J0#forceEdit=true&offline=true&sandboxMode=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Sanity Check\n",
    "\n",
    "Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.\n",
    "\n",
    "Check the output of `!nvidia-smi` to make sure you've been allocated a Tesla T4, P4, or P100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup:\n",
    "Set up script installs\n",
    "1. Updates gcc in Colab\n",
    "1. Installs Conda\n",
    "1. Install RAPIDS' current stable version of its libraries, as well as some external libraries including:\n",
    "     1. cuDF\n",
    "     2. cuML\n",
    "     3. cuGraph\n",
    "     4. cuSpatial\n",
    "     5. cuSignal\n",
    "     6. BlazingSQL\n",
    "     7. xgboost\n",
    "1. Copy RAPIDS .so files into current working directory, a neccessary workaround for RAPIDS+Colab integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
    "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
    "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "!python rapidsai-csp-utils/colab/env-check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will update the Colab environment and restart the kernel. Don't run the next cell until you see the session crash.\n",
    "!bash rapidsai-csp-utils/colab/update_gcc.sh\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will install CondaColab. This will restart your kernel one last time. Run this cell by itself and only run the next cell once you see the session crash.\n",
    "import condacolab\n",
    "condacolab.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now run the rest of the cells as normal\n",
    "import condacolab\n",
    "condacolab.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing RAPIDS is now 'python rapidsai-csp-utils/colab/install_rapids.py <release> <packages>'\n",
    "# The <release> options are 'stable' and 'nightly'.  Leaving it blank or adding any other words will default to stable.\n",
    "!python rapidsai-csp-utils/colab/install_rapids.py stable\n",
    "import os\n",
    "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
    "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
    "os.environ['CONDA_PREFIX'] = '/usr/local'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate visualizations of classification results\n",
    "\n",
    "import os\n",
    "from sklearn import metrics\n",
    "import  scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def svm_feature_importance(coef, names, fig_path, plot_title):\n",
    "    \"\"\"Visualize the feature importance of the SVM classifier.\"\"\"\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    # Plot the feature importance\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.title(f'{plot_title} Feature Importance')\n",
    "    plt.savefig(os.path.join(fig_path, 'feature_importance.png'))\n",
    "\n",
    "\n",
    "def visualize_results(results, classifier_name):\n",
    "    \"\"\"Visualize ROC Curve, Confusion Matrix, Classification Report and Feature Importance.\"\"\"\n",
    "    feature_names = results['x_train'].columns.tolist()\n",
    "    # Derive plot titles from classifier name. Make first letter uppercase and replace underscores with spaces\n",
    "    plot_title = classifier_name.title().replace('_', ' ')\n",
    "    # Join classifier_name with images_path\n",
    "    fig_path = os.path.join('images', classifier_name)\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(fig_path):\n",
    "        os.makedirs(fig_path)\n",
    "    # Feature Importance\n",
    "    # If Classifier name has svm, use a different method to visualize feature importance\n",
    "    if 'linear_svm' == classifier_name:\n",
    "        svm_feature_importance(results['model'].coef_[0], feature_names, fig_path, plot_title)\n",
    "    elif 'decision_tree' in classifier_name:\n",
    "        skplt.estimators.plot_feature_importances(results['model'], feature_names=results['x_train'].columns, figsize=(14,6))\n",
    "        plt.title(f'{plot_title} Feature Importance')\n",
    "        plt.savefig(os.path.join(fig_path, 'feature_importance.png'))\n",
    "    # ROC Curve\n",
    "    roc_curve = metrics.RocCurveDisplay.from_predictions(results['y_test'], results['y_pred'])\n",
    "    roc_curve.plot()\n",
    "    plt.title(f'{plot_title} ROC Curve')\n",
    "    plt.savefig(os.path.join(fig_path, 'roc_curve.png'))\n",
    "    # Confusion Matrix\n",
    "    skplt.metrics.plot_confusion_matrix(results['y_test'], results['y_pred'])\n",
    "    plt.title(f'{plot_title} Confusion Matrix')\n",
    "    plt.savefig(os.path.join(fig_path, 'confusion_matrix.png'))\n",
    "    # Plot Precision-Recall Curve\n",
    "    precision, recall, _ = metrics.precision_recall_curve(results['y_test'], results['y_pred'])\n",
    "    precision_recall_display = metrics.PrecisionRecallDisplay(precision, recall)\n",
    "    precision_recall_display.plot()\n",
    "    plt.title(f'{plot_title} Precision-Recall Curve')\n",
    "    plt.savefig(os.path.join(fig_path, 'precision_recall_curve.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate reports for each model\n",
    "\n",
    "def generate_report(clf, classifier_name):\n",
    "\n",
    "    print(f\"{classifier_name} Results\\n\" + \"-\" * 50 + \"\\n\")\n",
    "    print('Accuracy: {:.4F} \\n'.format(metrics.accuracy_score(clf['y_test'], clf['y_pred']) * 100))\n",
    "    print('Confusion Matrix: \\n', metrics.confusion_matrix(clf['y_test'], clf['y_pred']))\n",
    "    print('Area under curve: {:.4F} \\n'.format(metrics.roc_auc_score(clf['y_test'], clf['y_pred'])))\n",
    "    print(metrics.classification_report(clf['y_test'], clf['y_pred']))\n",
    "    print(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Metadata for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load Data and generate some metadata regarding the classes\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the file from the data directory\n",
    "af_data = pd.read_csv('data/Preprocessed_AFData.csv')\n",
    "# Print some metadata about the classes\n",
    "(unique,counts) = np.unique(af_data['Control'],return_counts=True)\n",
    "print(\"Metadata:\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(f\"Classes: {str(unique)} \\n\")\n",
    "print(\"Class Labels: \\n 0 - Non AF\\n 1 - AF\\n\")\n",
    "print(f\"Data in the 'Control' column: {dict(zip(unique,counts))} \\n\")\n",
    "print(f\"Ratio of occurrences of each class: {dict(zip(unique,counts/len(af_data['Control'])))}\\n\")\n",
    "print(\"-\"*50 + \"\\n\")\n",
    "# Display counts on a graph and save it\n",
    "target_variables = ['Non-AF','AF']\n",
    "sns.set_theme(style='whitegrid')\n",
    "sns.barplot(x=target_variables,y=counts)\n",
    "plt.title('Count of AF and Non-AF occurrences in the Preprocessed data')\n",
    "plt.ylabel('Count')\n",
    "for i,_ in enumerate(counts):\n",
    "    plt.text(i-0.25, counts[i]+0.5, counts[i], color='black', fontweight='bold')\n",
    "img_file_path = 'images/general/count_of_AF_and_Non_AF_occurrences.png'\n",
    "plt.savefig(img_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = af_data.drop(['Control'],axis=1)\n",
    "y = af_data['Control']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83bb4f012846a47f4d52528631dd63e6cfba4dd4be145200a10778e885aa240e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
